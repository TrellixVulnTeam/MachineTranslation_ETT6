This repository holds the code for Machine Translation Project. 

This is work under progress and will be updated regularly.

5 Nov 2020: The repo currently holds the code for pre-training the data with ELECTRA.

Folder Structure:
1. bert-vocab-builder:
   - This is a clone of the repo https://github.com/kwonmha/bert-vocab-builder. This takes in the corpus and generates the vocab. This code is not used in the set up currently. Huggingface tokenizer library is used instead.

2. data_small
    - This directory contains the corpus. A small part of the data from WMT20 is included in data_small/corpus.
    - The vocab generated is also stored in this folder as wordpiece_vocab.txt.
    - data_small/models contain the output model generated by electra.

3. electra
    - This is the electra code cloned from https://github.com/google-research/electra.

4. output
    - The (intermediate) output generated by electra is stored here.

5. VT_scripts
This folder conatins the scripts to run the workflow. The list of scripts are:
  1. tokenize.py - This invokes the huggingface tokenizer library to generate the wordpiece vocab. the script needs as input the path to the corpus, output path and the voca size.
  2. gen_wordpiece_vocab.sh - This invokes the tokenize.py script with the right arguments to generate the vocab at the path data_small/wordpiece_vocab.txt.
  3. electra_build_dataset.sh - It invokes the build_pretraining_dataset.py script from the electra library with the required parameters.
  4. electra_run_pretraining.sh - It invokes the run_pretraining.py script from the electra library with the required parameters.
  Note: Note that the *.sh scripts sets up teh conda environmnet in them. If you are using a differnt environment name, remember to change it in the script.
  5. electra_config.json - The important configurations for the electra training is added here.
  6. *.cmd scripts are scripts for running the scripts on condor framework (on patas machine).

  Environment Set-up:
  Set up conda environment with the following installed:
  - Tensorflow 1.15
  - pip install tokenizers
  - numpy
  - scikit-learn


  Workflow:
   - Run ./gen_wordpiece_vocab.sh
      By default, it assumes teh corpus is placed in data_small/corpus.
   - Run ./electra_build_dataset.sh
   - Run ./electra_run_pretraining.sh
      Edit the electra_config.json file if required. 
   - (To run in the condor set up on patas machine, use the *.cmd scripts)  